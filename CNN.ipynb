{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz1fEoAhtsgZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "199c8018-7a6d-4698-d081-47c67a6a2d52"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHfj--Qjtqcj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "cf64b108-94a9-4c1a-e9cd-2479f2a7d0b8"
      },
      "source": [
        "\n",
        "######\n",
        "#neural net example\n",
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch Version:  1.3.1+cu100\n",
            "Torchvision Version:  0.4.2+cu100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7Lvil68tqcp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Top level data directory. Here we assume the format of the directory conforms\n",
        "#   to the ImageFolder structure\n",
        "#note that the training set of data should be in a folder called train\n",
        "#the validation set data should be in a folder called val. inside\n",
        "# train and val there should be two folders named after\n",
        "# our classes. One called alcoholic with the alcoholics data and control with control\n",
        "# data. ImageFolder loader will automatically assign and keep track of labels.\n",
        "#will automatically assign labels\n",
        "data_dir = \"/content/drive/My Drive/SMNI_CMI_TRAIN_unzipped/SMNI_CNN/Images\"\n",
        "\n",
        "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
        "model_name = \"inception\"\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = 2\n",
        "\n",
        "# Batch size for training (change depending on how much memory you have)\n",
        "batch_size = 32\n",
        "\n",
        "# Number of epochs to train for\n",
        "num_epochs = 100\n",
        "\n",
        "# Flag for feature extracting. When False, we finetune the whole model,\n",
        "#   when True we only update the reshaped layer params\n",
        "feature_extract = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mSP-6xJJ_ED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=True):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "                    # Special case for inception because in training it has an auxiliary output. In train\n",
        "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
        "                    #   but in testing we only consider the final output.\n",
        "                    if is_inception and phase == 'train':\n",
        "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
        "                        outputs, aux_outputs = model(inputs)\n",
        "                        loss1 = criterion(outputs, labels)\n",
        "                        loss2 = criterion(aux_outputs, labels)\n",
        "                        loss = loss1 + 0.4*loss2\n",
        "                    else:\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'val':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    # print('Best http://localhost:8888/notebooks/Inception_V3_Model_Code.ipynb#val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKuBUGt83utU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zevs4b52tqcs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    if model_name == \"inception\":\n",
        "        \"\"\" Inception v3\n",
        "        Be careful, expects (299,299) sized images and has auxiliary output\n",
        "        \"\"\"\n",
        "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        # Handle the auxilary net\n",
        "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
        "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        # Handle the primary net\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 299\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid model name, exiting...\")\n",
        "        exit()\n",
        "\n",
        "    return model_ft, input_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLsd8IUAtqcv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUMtAzL3tqcy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6f4c642f-00d9-4429-f03e-35bfcd6d0cd4"
      },
      "source": [
        "# Initialize the model for this run\n",
        "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "\n",
        "# Print the model we just instantiated\n",
        "print(model_ft)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inception3(\n",
            "  (Conv2d_1a_3x3): BasicConv2d(\n",
            "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (Conv2d_2a_3x3): BasicConv2d(\n",
            "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (Conv2d_2b_3x3): BasicConv2d(\n",
            "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (Conv2d_3b_1x1): BasicConv2d(\n",
            "    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (Conv2d_4a_3x3): BasicConv2d(\n",
            "    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (Mixed_5b): InceptionA(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_1): BasicConv2d(\n",
            "      (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_2): BasicConv2d(\n",
            "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_5c): InceptionA(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_1): BasicConv2d(\n",
            "      (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_2): BasicConv2d(\n",
            "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_5d): InceptionA(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_1): BasicConv2d(\n",
            "      (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_2): BasicConv2d(\n",
            "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_6a): InceptionB(\n",
            "    (branch3x3): BasicConv2d(\n",
            "      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_6b): InceptionC(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_2): BasicConv2d(\n",
            "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_3): BasicConv2d(\n",
            "      (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_4): BasicConv2d(\n",
            "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_5): BasicConv2d(\n",
            "      (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_6c): InceptionC(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_2): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_3): BasicConv2d(\n",
            "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_4): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_5): BasicConv2d(\n",
            "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_6d): InceptionC(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_2): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_3): BasicConv2d(\n",
            "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_4): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_5): BasicConv2d(\n",
            "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_6e): InceptionC(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_2): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_3): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_4): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_5): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (AuxLogits): InceptionAux(\n",
            "    (conv0): BasicConv2d(\n",
            "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (conv1): BasicConv2d(\n",
            "      (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (fc): Linear(in_features=768, out_features=2, bias=True)\n",
            "  )\n",
            "  (Mixed_7a): InceptionD(\n",
            "    (branch3x3_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_2): BasicConv2d(\n",
            "      (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7x3_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7x3_2): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7x3_3): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7x3_4): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_7b): InceptionE(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_1): BasicConv2d(\n",
            "      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_2a): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_2b): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3a): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3b): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_7c): InceptionE(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_1): BasicConv2d(\n",
            "      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_2a): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_2b): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3a): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3b): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (fc): Linear(in_features=2048, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i78zR0aDKMUE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a9451696-4c6e-4786-da74-0323d53de27f"
      },
      "source": [
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(input_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "print(\"Initializing Datasets and Dataloaders...\")\n",
        "\n",
        "# Create training and validation datasets\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
        "# Create training and validation dataloaders\n",
        "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
        "# Detect if we have a GPU available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initializing Datasets and Dataloaders...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auGUk9-sLb-j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a9d8afd3-df73-44c6-9560-58e981a1e955"
      },
      "source": [
        "print(device)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mb5Xvh8qKj8F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "368d7ef8-f9b7-47c3-e300-3dfee71645a8"
      },
      "source": [
        "# Send the model to GPU\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# Gather the parameters to be optimized/updated in this run. If we are\n",
        "#  finetuning we will be updating all parameters. However, if we are\n",
        "#  doing feature extract method, we will only update the parameters\n",
        "#  that we have just initialized, i.e. the parameters with requires_grad\n",
        "#  is True.\n",
        "params_to_update = model_ft.parameters()\n",
        "print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "#optimizer_ft = optim.SGD(params_to_update, lr=0.0001, momentum=0.9)\n",
        "optimizer_ft = optim.RMSprop(params_to_update, lr = 0.045, alpha = 0.9,eps=1,weight_decay=0.94)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Params to learn:\n",
            "\t AuxLogits.fc.weight\n",
            "\t AuxLogits.fc.bias\n",
            "\t fc.weight\n",
            "\t fc.bias\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp5XKNeyKl88",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9a97a5a1-ac0b-494f-9fa6-ef107624f5dc"
      },
      "source": [
        "###############################\n",
        "#note that if model_ft, hist = train_model(...) crashes with\n",
        "# try removing one image from the training set\n",
        "#https://forums.fast.ai/t/understanding-code-error-expected-more-than-1-value-per-channel-when-training/9257/10\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train and evaluate\n",
        "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/99\n",
            "----------\n",
            "train Loss: 1.8251 Acc: 0.4950\n",
            "val Loss: 2.5613 Acc: 0.5000\n",
            "\n",
            "Epoch 1/99\n",
            "----------\n",
            "train Loss: 1.6973 Acc: 0.5325\n",
            "val Loss: 1.4419 Acc: 0.5000\n",
            "\n",
            "Epoch 2/99\n",
            "----------\n",
            "train Loss: 2.0934 Acc: 0.4700\n",
            "val Loss: 0.7469 Acc: 0.5000\n",
            "\n",
            "Epoch 3/99\n",
            "----------\n",
            "train Loss: 1.9236 Acc: 0.4900\n",
            "val Loss: 1.8352 Acc: 0.5000\n",
            "\n",
            "Epoch 4/99\n",
            "----------\n",
            "train Loss: 2.2432 Acc: 0.4900\n",
            "val Loss: 0.6763 Acc: 0.5700\n",
            "\n",
            "Epoch 5/99\n",
            "----------\n",
            "train Loss: 1.4057 Acc: 0.5325\n",
            "val Loss: 2.1633 Acc: 0.5000\n",
            "\n",
            "Epoch 6/99\n",
            "----------\n",
            "train Loss: 1.9236 Acc: 0.4975\n",
            "val Loss: 0.8546 Acc: 0.5000\n",
            "\n",
            "Epoch 7/99\n",
            "----------\n",
            "train Loss: 1.9459 Acc: 0.4925\n",
            "val Loss: 0.9130 Acc: 0.5000\n",
            "\n",
            "Epoch 8/99\n",
            "----------\n",
            "train Loss: 2.1224 Acc: 0.4500\n",
            "val Loss: 2.6225 Acc: 0.5000\n",
            "\n",
            "Epoch 9/99\n",
            "----------\n",
            "train Loss: 2.1204 Acc: 0.4650\n",
            "val Loss: 2.9414 Acc: 0.5000\n",
            "\n",
            "Epoch 10/99\n",
            "----------\n",
            "train Loss: 2.2022 Acc: 0.4775\n",
            "val Loss: 2.6657 Acc: 0.5000\n",
            "\n",
            "Epoch 11/99\n",
            "----------\n",
            "train Loss: 1.8796 Acc: 0.5075\n",
            "val Loss: 1.2764 Acc: 0.5000\n",
            "\n",
            "Epoch 12/99\n",
            "----------\n",
            "train Loss: 2.0591 Acc: 0.4800\n",
            "val Loss: 1.0379 Acc: 0.5000\n",
            "\n",
            "Epoch 13/99\n",
            "----------\n",
            "train Loss: 1.9006 Acc: 0.4850\n",
            "val Loss: 1.0596 Acc: 0.5000\n",
            "\n",
            "Epoch 14/99\n",
            "----------\n",
            "train Loss: 2.1269 Acc: 0.4700\n",
            "val Loss: 0.7144 Acc: 0.5250\n",
            "\n",
            "Epoch 15/99\n",
            "----------\n",
            "train Loss: 1.7078 Acc: 0.5300\n",
            "val Loss: 1.9168 Acc: 0.5000\n",
            "\n",
            "Epoch 16/99\n",
            "----------\n",
            "train Loss: 2.0392 Acc: 0.4900\n",
            "val Loss: 1.2883 Acc: 0.5000\n",
            "\n",
            "Epoch 17/99\n",
            "----------\n",
            "train Loss: 1.6434 Acc: 0.5375\n",
            "val Loss: 0.7676 Acc: 0.5100\n",
            "\n",
            "Epoch 18/99\n",
            "----------\n",
            "train Loss: 1.9850 Acc: 0.4950\n",
            "val Loss: 0.9887 Acc: 0.5000\n",
            "\n",
            "Epoch 19/99\n",
            "----------\n",
            "train Loss: 1.4979 Acc: 0.5650\n",
            "val Loss: 0.8329 Acc: 0.5000\n",
            "\n",
            "Epoch 20/99\n",
            "----------\n",
            "train Loss: 1.4870 Acc: 0.5650\n",
            "val Loss: 0.8975 Acc: 0.5000\n",
            "\n",
            "Epoch 21/99\n",
            "----------\n",
            "train Loss: 1.7583 Acc: 0.5025\n",
            "val Loss: 1.6372 Acc: 0.5000\n",
            "\n",
            "Epoch 22/99\n",
            "----------\n",
            "train Loss: 2.0418 Acc: 0.4950\n",
            "val Loss: 2.2922 Acc: 0.5000\n",
            "\n",
            "Epoch 23/99\n",
            "----------\n",
            "train Loss: 1.8366 Acc: 0.5050\n",
            "val Loss: 3.2105 Acc: 0.5000\n",
            "\n",
            "Epoch 24/99\n",
            "----------\n",
            "train Loss: 1.8720 Acc: 0.5150\n",
            "val Loss: 0.9186 Acc: 0.5000\n",
            "\n",
            "Epoch 25/99\n",
            "----------\n",
            "train Loss: 1.9252 Acc: 0.4850\n",
            "val Loss: 1.6632 Acc: 0.5000\n",
            "\n",
            "Epoch 26/99\n",
            "----------\n",
            "train Loss: 2.0047 Acc: 0.4800\n",
            "val Loss: 2.9581 Acc: 0.5000\n",
            "\n",
            "Epoch 27/99\n",
            "----------\n",
            "train Loss: 1.8965 Acc: 0.5150\n",
            "val Loss: 1.6870 Acc: 0.5000\n",
            "\n",
            "Epoch 28/99\n",
            "----------\n",
            "train Loss: 2.0432 Acc: 0.4750\n",
            "val Loss: 0.7364 Acc: 0.5150\n",
            "\n",
            "Epoch 29/99\n",
            "----------\n",
            "train Loss: 1.6770 Acc: 0.5250\n",
            "val Loss: 0.8903 Acc: 0.5000\n",
            "\n",
            "Epoch 30/99\n",
            "----------\n",
            "train Loss: 2.0259 Acc: 0.4900\n",
            "val Loss: 1.7886 Acc: 0.5000\n",
            "\n",
            "Epoch 31/99\n",
            "----------\n",
            "train Loss: 2.0279 Acc: 0.4875\n",
            "val Loss: 2.5005 Acc: 0.5000\n",
            "\n",
            "Epoch 32/99\n",
            "----------\n",
            "train Loss: 1.7189 Acc: 0.5325\n",
            "val Loss: 4.2339 Acc: 0.5000\n",
            "\n",
            "Epoch 33/99\n",
            "----------\n",
            "train Loss: 2.3160 Acc: 0.4700\n",
            "val Loss: 0.8711 Acc: 0.5000\n",
            "\n",
            "Epoch 34/99\n",
            "----------\n",
            "train Loss: 1.8024 Acc: 0.4900\n",
            "val Loss: 1.6254 Acc: 0.5000\n",
            "\n",
            "Epoch 35/99\n",
            "----------\n",
            "train Loss: 1.9150 Acc: 0.5000\n",
            "val Loss: 2.7451 Acc: 0.5000\n",
            "\n",
            "Epoch 36/99\n",
            "----------\n",
            "train Loss: 1.6921 Acc: 0.5300\n",
            "val Loss: 1.3685 Acc: 0.5000\n",
            "\n",
            "Epoch 37/99\n",
            "----------\n",
            "train Loss: 1.9984 Acc: 0.4825\n",
            "val Loss: 1.0815 Acc: 0.5000\n",
            "\n",
            "Epoch 38/99\n",
            "----------\n",
            "train Loss: 1.9385 Acc: 0.4950\n",
            "val Loss: 0.6963 Acc: 0.5500\n",
            "\n",
            "Epoch 39/99\n",
            "----------\n",
            "train Loss: 1.7572 Acc: 0.5150\n",
            "val Loss: 0.7574 Acc: 0.5100\n",
            "\n",
            "Epoch 40/99\n",
            "----------\n",
            "train Loss: 2.3663 Acc: 0.4500\n",
            "val Loss: 3.0248 Acc: 0.5000\n",
            "\n",
            "Epoch 41/99\n",
            "----------\n",
            "train Loss: 1.7401 Acc: 0.5275\n",
            "val Loss: 1.2937 Acc: 0.5000\n",
            "\n",
            "Epoch 42/99\n",
            "----------\n",
            "train Loss: 1.9283 Acc: 0.5000\n",
            "val Loss: 1.3143 Acc: 0.5000\n",
            "\n",
            "Epoch 43/99\n",
            "----------\n",
            "train Loss: 1.9631 Acc: 0.5000\n",
            "val Loss: 0.8098 Acc: 0.5000\n",
            "\n",
            "Epoch 44/99\n",
            "----------\n",
            "train Loss: 1.6726 Acc: 0.5300\n",
            "val Loss: 1.5816 Acc: 0.5000\n",
            "\n",
            "Epoch 45/99\n",
            "----------\n",
            "train Loss: 2.3838 Acc: 0.4650\n",
            "val Loss: 0.8019 Acc: 0.5000\n",
            "\n",
            "Epoch 46/99\n",
            "----------\n",
            "train Loss: 1.8339 Acc: 0.5100\n",
            "val Loss: 0.6662 Acc: 0.6100\n",
            "\n",
            "Epoch 47/99\n",
            "----------\n",
            "train Loss: 1.5126 Acc: 0.5275\n",
            "val Loss: 1.1533 Acc: 0.5000\n",
            "\n",
            "Epoch 48/99\n",
            "----------\n",
            "train Loss: 1.4618 Acc: 0.5725\n",
            "val Loss: 1.0178 Acc: 0.5000\n",
            "\n",
            "Epoch 49/99\n",
            "----------\n",
            "train Loss: 1.6017 Acc: 0.5200\n",
            "val Loss: 2.2809 Acc: 0.5000\n",
            "\n",
            "Epoch 50/99\n",
            "----------\n",
            "train Loss: 1.7710 Acc: 0.5275\n",
            "val Loss: 0.8553 Acc: 0.5000\n",
            "\n",
            "Epoch 51/99\n",
            "----------\n",
            "train Loss: 2.3332 Acc: 0.4500\n",
            "val Loss: 1.5998 Acc: 0.5000\n",
            "\n",
            "Epoch 52/99\n",
            "----------\n",
            "train Loss: 1.6883 Acc: 0.5250\n",
            "val Loss: 1.3390 Acc: 0.5000\n",
            "\n",
            "Epoch 53/99\n",
            "----------\n",
            "train Loss: 1.6054 Acc: 0.5350\n",
            "val Loss: 2.0836 Acc: 0.5000\n",
            "\n",
            "Epoch 54/99\n",
            "----------\n",
            "train Loss: 1.6766 Acc: 0.5375\n",
            "val Loss: 0.9376 Acc: 0.5000\n",
            "\n",
            "Epoch 55/99\n",
            "----------\n",
            "train Loss: 1.6895 Acc: 0.5200\n",
            "val Loss: 1.6695 Acc: 0.5000\n",
            "\n",
            "Epoch 56/99\n",
            "----------\n",
            "train Loss: 1.8285 Acc: 0.5250\n",
            "val Loss: 1.7090 Acc: 0.5000\n",
            "\n",
            "Epoch 57/99\n",
            "----------\n",
            "train Loss: 1.5852 Acc: 0.5475\n",
            "val Loss: 0.7879 Acc: 0.5000\n",
            "\n",
            "Epoch 58/99\n",
            "----------\n",
            "train Loss: 2.0132 Acc: 0.4800\n",
            "val Loss: 1.1181 Acc: 0.5000\n",
            "\n",
            "Epoch 59/99\n",
            "----------\n",
            "train Loss: 1.7961 Acc: 0.5050\n",
            "val Loss: 1.4875 Acc: 0.5000\n",
            "\n",
            "Epoch 60/99\n",
            "----------\n",
            "train Loss: 1.6594 Acc: 0.5425\n",
            "val Loss: 1.7286 Acc: 0.5000\n",
            "\n",
            "Epoch 61/99\n",
            "----------\n",
            "train Loss: 1.7778 Acc: 0.5200\n",
            "val Loss: 0.6747 Acc: 0.6250\n",
            "\n",
            "Epoch 62/99\n",
            "----------\n",
            "train Loss: 1.8617 Acc: 0.5075\n",
            "val Loss: 0.8839 Acc: 0.5000\n",
            "\n",
            "Epoch 63/99\n",
            "----------\n",
            "train Loss: 1.9597 Acc: 0.4800\n",
            "val Loss: 1.4027 Acc: 0.5000\n",
            "\n",
            "Epoch 64/99\n",
            "----------\n",
            "train Loss: 1.7352 Acc: 0.5350\n",
            "val Loss: 0.7709 Acc: 0.5050\n",
            "\n",
            "Epoch 65/99\n",
            "----------\n",
            "train Loss: 1.6076 Acc: 0.5325\n",
            "val Loss: 0.7902 Acc: 0.5050\n",
            "\n",
            "Epoch 66/99\n",
            "----------\n",
            "train Loss: 2.1924 Acc: 0.4600\n",
            "val Loss: 1.4333 Acc: 0.5000\n",
            "\n",
            "Epoch 67/99\n",
            "----------\n",
            "train Loss: 1.6917 Acc: 0.5250\n",
            "val Loss: 1.6075 Acc: 0.5000\n",
            "\n",
            "Epoch 68/99\n",
            "----------\n",
            "train Loss: 1.4902 Acc: 0.5325\n",
            "val Loss: 4.2176 Acc: 0.5000\n",
            "\n",
            "Epoch 69/99\n",
            "----------\n",
            "train Loss: 1.5886 Acc: 0.5650\n",
            "val Loss: 2.5845 Acc: 0.5000\n",
            "\n",
            "Epoch 70/99\n",
            "----------\n",
            "train Loss: 2.4787 Acc: 0.4500\n",
            "val Loss: 1.5900 Acc: 0.5000\n",
            "\n",
            "Epoch 71/99\n",
            "----------\n",
            "train Loss: 2.0991 Acc: 0.4850\n",
            "val Loss: 1.5148 Acc: 0.5000\n",
            "\n",
            "Epoch 72/99\n",
            "----------\n",
            "train Loss: 1.9167 Acc: 0.4975\n",
            "val Loss: 1.2580 Acc: 0.5000\n",
            "\n",
            "Epoch 73/99\n",
            "----------\n",
            "train Loss: 1.7819 Acc: 0.5250\n",
            "val Loss: 0.6596 Acc: 0.6000\n",
            "\n",
            "Epoch 74/99\n",
            "----------\n",
            "train Loss: 1.8792 Acc: 0.5050\n",
            "val Loss: 1.1122 Acc: 0.5000\n",
            "\n",
            "Epoch 75/99\n",
            "----------\n",
            "train Loss: 2.2628 Acc: 0.4500\n",
            "val Loss: 1.5443 Acc: 0.5000\n",
            "\n",
            "Epoch 76/99\n",
            "----------\n",
            "train Loss: 1.9102 Acc: 0.5000\n",
            "val Loss: 1.1473 Acc: 0.5000\n",
            "\n",
            "Epoch 77/99\n",
            "----------\n",
            "train Loss: 1.6892 Acc: 0.5200\n",
            "val Loss: 1.3638 Acc: 0.5000\n",
            "\n",
            "Epoch 78/99\n",
            "----------\n",
            "train Loss: 2.3042 Acc: 0.4550\n",
            "val Loss: 2.4025 Acc: 0.5000\n",
            "\n",
            "Epoch 79/99\n",
            "----------\n",
            "train Loss: 2.1381 Acc: 0.4700\n",
            "val Loss: 1.0403 Acc: 0.5000\n",
            "\n",
            "Epoch 80/99\n",
            "----------\n",
            "train Loss: 2.2339 Acc: 0.4650\n",
            "val Loss: 1.2747 Acc: 0.5000\n",
            "\n",
            "Epoch 81/99\n",
            "----------\n",
            "train Loss: 2.0057 Acc: 0.4750\n",
            "val Loss: 2.9178 Acc: 0.5000\n",
            "\n",
            "Epoch 82/99\n",
            "----------\n",
            "train Loss: 1.6968 Acc: 0.5350\n",
            "val Loss: 0.8242 Acc: 0.5000\n",
            "\n",
            "Epoch 83/99\n",
            "----------\n",
            "train Loss: 1.9160 Acc: 0.4925\n",
            "val Loss: 2.6224 Acc: 0.5000\n",
            "\n",
            "Epoch 84/99\n",
            "----------\n",
            "train Loss: 2.1300 Acc: 0.4725\n",
            "val Loss: 2.6908 Acc: 0.5000\n",
            "\n",
            "Epoch 85/99\n",
            "----------\n",
            "train Loss: 1.7780 Acc: 0.5200\n",
            "val Loss: 2.2398 Acc: 0.5000\n",
            "\n",
            "Epoch 86/99\n",
            "----------\n",
            "train Loss: 2.2018 Acc: 0.4700\n",
            "val Loss: 0.9965 Acc: 0.5000\n",
            "\n",
            "Epoch 87/99\n",
            "----------\n",
            "train Loss: 1.8601 Acc: 0.4925\n",
            "val Loss: 1.8785 Acc: 0.5000\n",
            "\n",
            "Epoch 88/99\n",
            "----------\n",
            "train Loss: 1.7521 Acc: 0.5125\n",
            "val Loss: 2.2354 Acc: 0.5000\n",
            "\n",
            "Epoch 89/99\n",
            "----------\n",
            "train Loss: 1.8756 Acc: 0.5075\n",
            "val Loss: 0.6850 Acc: 0.5350\n",
            "\n",
            "Epoch 90/99\n",
            "----------\n",
            "train Loss: 1.9104 Acc: 0.4975\n",
            "val Loss: 1.8209 Acc: 0.5000\n",
            "\n",
            "Epoch 91/99\n",
            "----------\n",
            "train Loss: 1.8226 Acc: 0.5100\n",
            "val Loss: 1.6212 Acc: 0.5000\n",
            "\n",
            "Epoch 92/99\n",
            "----------\n",
            "train Loss: 2.1108 Acc: 0.4750\n",
            "val Loss: 2.1595 Acc: 0.5000\n",
            "\n",
            "Epoch 93/99\n",
            "----------\n",
            "train Loss: 1.6203 Acc: 0.5325\n",
            "val Loss: 0.6503 Acc: 0.6550\n",
            "\n",
            "Epoch 94/99\n",
            "----------\n",
            "train Loss: 1.6772 Acc: 0.5125\n",
            "val Loss: 2.0835 Acc: 0.5000\n",
            "\n",
            "Epoch 95/99\n",
            "----------\n",
            "train Loss: 1.6059 Acc: 0.5600\n",
            "val Loss: 0.7835 Acc: 0.5000\n",
            "\n",
            "Epoch 96/99\n",
            "----------\n",
            "train Loss: 1.5071 Acc: 0.5700\n",
            "val Loss: 1.2330 Acc: 0.5000\n",
            "\n",
            "Epoch 97/99\n",
            "----------\n",
            "train Loss: 2.0598 Acc: 0.4750\n",
            "val Loss: 0.7268 Acc: 0.5150\n",
            "\n",
            "Epoch 98/99\n",
            "----------\n",
            "train Loss: 1.5989 Acc: 0.5300\n",
            "val Loss: 1.4019 Acc: 0.5000\n",
            "\n",
            "Epoch 99/99\n",
            "----------\n",
            "train Loss: 1.5369 Acc: 0.5750\n",
            "val Loss: 1.6724 Acc: 0.5000\n",
            "\n",
            "Training complete in 12m 26s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZi_cVz0LlU1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}